# LoRA Training Plan

## Goal
Establish a reliable, high-quality workflow to train Low-Rank Adaptation (LoRA) models for characters generated by the Director's Chair.

## Landscape Analysis (2025 State of the Art)

Training Flux-based models (which `z-image-turbo` is based on) has matured significantly.

### 1. Local Training on Apple Silicon (Recommended)
Since you are on an M3 Mac, we have a distinct advantage: **Unified Memory**.
*   **Engine:** `mflux` (MLX Flux).
*   **Pros:** 
    *   Native performance on Apple Silicon.
    *   No data privacy concerns (images stay local).
    *   Free (no cloud costs).
    *   We already use `mflux` for generation, keeping dependencies minimal.
*   **Cons:** 
    *   Slower than H100 cloud clusters.
    *   Requires significant RAM (16GB minimum, 32GB+ recommended for 4-bit/8-bit quantized training).

### 2. Cloud Training (Fal.ai / Replicate)
*   **Pros:** Fast (minutes), guaranteed H100 performance.
*   **Cons:** Cost per run, requires uploading datasets, adds API complexity.

### 3. PC/Nvidia Tools (Kohya_ss, AI-Toolkit)
*   **Context:** These are the industry standards for Nvidia GPUs. `AI-Toolkit` (Ostris) is currently considered SOTA for Flux LoRA quality.
*   **Relevance:** `mflux` roughly matches these in methodology but is optimized for Mac.

## Future Hardware Scalability (DGX / Nvidia)
*Speculation: User may acquire an Nvidia DGX workstation in late Feb 2025.*

To support this transition without rewriting the entire application, we will adopt a **Multi-Engine Architecture** for training.

1.  **Standardized Dataset:** The `Image + .txt Caption` format is universal. It works for `mflux`, `kohya_ss`, and `AI-Toolkit`. This remains our core data contract.
2.  **Engine Abstraction:** 
    *   The CLI (`train_lora.py`) will not call `mflux` directly.
    *   It will delegate to a `TrainingEngine` interface.
    *   **Engine A (Initial):** `MFluxEngine` (Native Apple Silicon).
    *   **Engine B (Future):** `NvidiaEngine` (likely wrapping `kohya_ss` or `diffusers` via Docker/Subprocess).
3.  **Path Forward:** We build strictly for M3/mflux today, but ensure imports and logic are isolated in `src/directors_chair/training/` to make swapping the backend trivial later.

## Selected Strategy: "Native Rails" (mflux)

We will implement a local training pipeline using `mflux`. This keeps the "Director's Chair" self-contained and free to use.

### The Workflow

1.  **Dataset Preparation (Automated)**
    *   We already generate images to `assets/training_data/{trigger}_{name}/`.
    *   **New Step:** We need to ensure a `metadata.jsonl` or sidecar `.txt` files exist for captions.
    *   *Constraint:* Flux training works best with natural language captions + a unique trigger word.
    *   *Action:* When generating training data in `cli.py`, we should auto-create the caption text files.

2.  **Configuration (Automated)**
    *   The user shouldn't hand-edit JSON config files.
    *   `scripts/train_lora.py` will ask a few questions (Rank, Epochs) and generate a temporary `training_config.json`.

3.  **Training (Execution)**
    *   We will invoke the `mflux.train` module programmatically or via subprocess.
    *   **Key Settings for M3:**
        *   `quantize`: 4 or 8 (crucial for memory).
        *   `lora_rank`: 16 (standard balance of size/quality).
        *   `batch_size`: 1 (to save VRAM).

4.  **Validation**
    *   After training, the `.safetensors` LoRA file is moved to `assets/loras/`.
    *   The user is prompted to run a "Test Scene" using the new LoRA.

## Implementation Steps for `scripts/train_lora.py`

1.  **Inputs:** 
    *   Target Character (from `assets/training_data`).
    *   LoRA Name.
2.  **Process:**
    *   Validate dataset (check for images + captions).
    *   Construct `mflux` Config Dictionary.
    *   Run Training Loop.
3.  **Output:**
    *   Save LoRA adapter to `assets/loras/{name}.safetensors`.
    *   Update `config.json` to register the new LoRA.

## Future Proofing
If local training proves too slow for your taste, we can add a `--cloud` flag to `train_lora.py` later to dispatch the job to Fal.ai. For now, we start local.
